{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XML Data\n",
    "\n",
    "## Check out XML & Python documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.w3.org/TR/xml/#sec-origin-goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing XML Data - couple different ways you can do it. By strings in multiple files or by files.\n",
    "## xml.etree.ElementTree as ET\n",
    "#### Element tree module reads the entire XML file in memory. Element objects allow us to iterate over the root elements to find their children and the tags associated with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.w3schools.com/xml/xpath_syntax.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print' (<ipython-input-1-6b1b0d20f8eb>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-6b1b0d20f8eb>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    print \"\\nTitle:\\n\", title_text\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'\n"
     ]
    }
   ],
   "source": [
    "## example code to explain parsing XML files\n",
    "## number of attributes for elements - tag & title are examples used\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import pprint\n",
    "\n",
    "tree = ET.parse('exampleResearchArticle.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "# like specifying a path for a file, you can specify a xml element\n",
    "# to look for\n",
    "title = root.find('./fm/bibl/title')\n",
    "title_text = \"\"\n",
    "for p in title:\n",
    "    title_text += p.text\n",
    "print (\"\\nTitle:\\n\", title_text)\n",
    "\n",
    "print(\"\\nAuthor email address:\")\n",
    "for a in root.findall('./fm/bibl/aug/au'):\n",
    "    email = a.find('email')\n",
    "    if email is not None:\n",
    "        print email.text\n",
    "        \n",
    "# .text grabs the value in the xml <tag>\n",
    "\n",
    "# to store in a dictionary, you have to file through the root\n",
    "# and store the values by .find & .text\n",
    "\n",
    "# EXAMPLE!\n",
    "\n",
    "# def get_root(fname):\n",
    "#     tree = ET.parse(fname)\n",
    "#     return tree.getroot()\n",
    "\n",
    "\n",
    "# def get_authors(root):\n",
    "#     authors = []\n",
    "#     for author in root.findall('./fm/bibl/aug/au'):\n",
    "#         data = {\n",
    "#                 \"fnm\": None,\n",
    "#                 \"snm\": None,\n",
    "#                 \"email\": None\n",
    "#         }\n",
    "\n",
    "#         data['fnm'] = author.find(\"fnm\").text\n",
    "#         data['snm'] = author.find(\"snm\").text\n",
    "#         data['email'] = author.find(\"email\").text\n",
    "        \n",
    "#         authors.append(data)\n",
    "\n",
    "#     return authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Screen Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inspecting the elements of a web page, you can see some of the data in the xml file\n",
    "\n",
    "# building list of values, making the http request to to download data, and then parsing the\n",
    "# data last is best practice for web scraping & parsing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-9471ab61e30a>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-9471ab61e30a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    form bs4 import BeautifulSoup\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "form bs4 import BeautifulSoup\n",
    "\n",
    "def options(soup, id):\n",
    "    options_values = []\n",
    "    # the carrier_list finds the 'CarrierList' element \n",
    "    # from the 'soup' variable called in main() \n",
    "    carrier_list = soup.find(id=id)\n",
    "    # find_all then makes a list of all 'option' descendents in the 'CarrierList'\n",
    "    # tag found from the Beautiful Soup html doc\n",
    "    for option in carrier_list.find_all('option'):\n",
    "        option_values.append(option['value'])\n",
    "    return option_values\n",
    "\n",
    "def print_list(label, codes):\n",
    "    print \"\\n%s:\" % label\n",
    "    for c in codes:\n",
    "        print (c)\n",
    "\n",
    "def main():\n",
    "    # file not available in computer\n",
    "    # BeautifulSoup passes the top level element for the html document\n",
    "    soup = BeautifulSoup(open(\"virgin_and_logan_airport.html\"))\n",
    "    \n",
    "    #using options function, all CarrierList variables are grabbed in a list\n",
    "    codes = options(soup, 'CarrierList')\n",
    "    print_list(\"Carriers\", codes)\n",
    "    \n",
    "    codes = options(soup, 'AirportList')\n",
    "    print_list(\"Airports\", codes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anytime you are doing a scraping task, you have to understand how a site expects requests. The first step is figuring out what url to access and what http method to use.  The latter is found in the \"method\" variable and the former is found by searching in the code. Sometimes it is obvious, sometimes it isn't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to look how we are going to prpogrammatically grab the data. The best way to do this is by looking at the browser and find out the requests it makes. By looking at the 'Network' tab, we can see a running tally of requests made. 'Post' can be found when this request is made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clicking on the post request, we can find what data is submitted when looking at the 'Form Data' section. Thinking that we would only need the carrier and airport fields, we find out that there are other sections needing to be submitted outside of carrier and airport."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz Below to grab all values from other tags - ex. 'EVENTVALIDATION'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "\n",
    "html_page = \"page_source.html\"\n",
    "\n",
    "\n",
    "def extract_data(page):\n",
    "    data = {\"eventvalidation\": \"\",\n",
    "            \"viewstate\": \"\"}\n",
    "    with open(page, \"r\") as html:\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        \n",
    "        ev = soup.find(id='__EVENTVALIDATION')\n",
    "        data['eventvalidation'] = ev[\"value\"]\n",
    "        \n",
    "        vs = soup.find(id='__VIEWSTATE')\n",
    "        data['viewstate'] = vs[\"value\"]\n",
    "        \n",
    "        pass\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def make_request(data):\n",
    "    eventvalidation = data[\"eventvalidation\"]\n",
    "    viewstate = data[\"viewstate\"]\n",
    "\n",
    "    r = requests.post(\"http://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "                    data={'AirportList': \"BOS\",\n",
    "                          'CarrierList': \"VX\",\n",
    "                          'Submit': 'Submit',\n",
    "                          \"__EVENTTARGET\": \"\",\n",
    "                          \"__EVENTARGUMENT\": \"\",\n",
    "                          \"__EVENTVALIDATION\": eventvalidation,\n",
    "                          \"__VIEWSTATE\": viewstate\n",
    "                    })\n",
    "\n",
    "    return r.text\n",
    "\n",
    "\n",
    "def test():\n",
    "    data = extract_data(html_page)\n",
    "    assert data[\"eventvalidation\"] != \"\"\n",
    "    assert data[\"eventvalidation\"].startswith(\"/wEWjAkCoIj1ng0\")\n",
    "    assert data[\"viewstate\"].startswith(\"/wEPDwUKLTI\")\n",
    "\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## populating eventvalidation and viewstate with list of all values for the tags in xml\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "s = requests.Session()\n",
    "\n",
    "r = s.get(\"http://www.transtats.bts.gov/Data_Elements.apx?Data=2\")\n",
    "soup = BeautifulSoup(r.text)\n",
    "viewstate_element = soup.find(id=\"__VIEWSTATE\")\n",
    "viewstate = viewstate_element[\"value\"]\n",
    "eventvalidation_element = soup.find(id=\"__EVENTVALIDATION\")\n",
    "eventvalidation = eventvalidationi_element[\"value\"]\n",
    "\n",
    "r = s.post(\"http://www.transtats.bts.gov//Data_Elements.aspx?Data=2\",\n",
    "                 data = {'AirportList' : 'BOS',\n",
    "                        'CarrierList' : \"VX\",\n",
    "                        'Submit' : \"Submit\",\n",
    "                        '__EVENTTARGET' : \"\",\n",
    "                        '__EVENTARGUMENT' : \"\",\n",
    "                        '__EVENTVALIDATION' : eventvalidation,\n",
    "                        '__VIEWSTATE' : viewstate})\n",
    "\n",
    "f = open(\"virgin_and_logan_airport.html\", \"w\")\n",
    "f.write(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices for Scraping\n",
    "\n",
    "### 1) Look at how a browser makes request\n",
    "### 2) Emulate in code\n",
    "### 3) If stuff blows up, look at your http traffic\n",
    "### 4) Return to 1 until it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Your task in this exercise is to modify 'extract_carrier()` to get a list of\n",
    "all airlines. Exclude all of the combination values like \"All U.S. Carriers\"\n",
    "from the data that you return. You should return a list of codes for the\n",
    "carriers.\n",
    "\n",
    "All your changes should be in the 'extract_carrier()' function. The\n",
    "'options.html' file in the tab above is a stripped down version of what is\n",
    "actually on the website, but should provide an example of what you should get\n",
    "from the full file.\n",
    "\n",
    "Please note that the function 'make_request()' is provided for your reference\n",
    "only. You will not be able to to actually use it from within the Udacity web UI.\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "html_page = \"options.html\"\n",
    "\n",
    "\n",
    "def extract_carriers(page):\n",
    "    data = []\n",
    "\n",
    "    with open(page, \"r\") as html:\n",
    "        # do something here to find the necessary values\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        carrier_list = soup.find(id=\"CarrierList\")\n",
    "        for carriers in carrier_list.find_all('option'):\n",
    "            if \"All\" not in carriers['value']:\n",
    "                data.append(carriers['value'])\n",
    "                print data\n",
    "        \n",
    "        # target = soup.find(id='__EVENTTARGET')\n",
    "        # data['eventtarget'] = target['value']\n",
    "        \n",
    "        # argument = soup.find(id='__EVENTARGUMENT')\n",
    "        # data['eventargument'] = argument['value']\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "def make_request(data):\n",
    "    eventvalidation = data[\"eventvalidation\"]\n",
    "    viewstate = data[\"viewstate\"]\n",
    "    airport = data[\"airport\"]\n",
    "    carrier = data[\"carrier\"]\n",
    "\n",
    "    r = s.post(\"https://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "               data = ((\"__EVENTTARGET\", \"\"),\n",
    "                       (\"__EVENTARGUMENT\", \"\"),\n",
    "                       (\"__VIEWSTATE\", viewstate),\n",
    "                       (\"__VIEWSTATEGENERATOR\",viewstategenerator),\n",
    "                       (\"__EVENTVALIDATION\", eventvalidation),\n",
    "                       (\"CarrierList\", carrier),\n",
    "                       (\"AirportList\", airport),\n",
    "                       (\"Submit\", \"Submit\")))\n",
    "\n",
    "    return r.text\n",
    "\n",
    "\n",
    "def test():\n",
    "    data = extract_carriers(html_page)\n",
    "    assert len(data) == 16\n",
    "    assert \"FL\" in data\n",
    "    assert \"NK\" in data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Complete the 'extract_airports()' function so that it returns a list of airport\n",
    "codes, excluding any combinations like \"All\".\n",
    "\n",
    "Refer to the 'options.html' file in the tab above for a stripped down version\n",
    "of what is actually on the website. The test() assertions are based on the\n",
    "given file.\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "html_page = \"options.html\"\n",
    "\n",
    "\n",
    "def extract_airports(page):\n",
    "    data = []\n",
    "    with open(page, \"r\") as html:\n",
    "        # do something here to find the necessary values\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        airports = soup.find(id='AirportList')\n",
    "        for airport in airports.find_all('option'):\n",
    "            if \"All\" not in airport['value']:\n",
    "                data.append(airport['value'])\n",
    "        for c in data:\n",
    "            print c\n",
    "    return data\n",
    "\n",
    "\n",
    "def test():\n",
    "    data = extract_airports(html_page)\n",
    "    assert len(data) == 15\n",
    "    assert \"ATL\" in data\n",
    "    assert \"ABR\" in data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 3 - this is impossible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-0e65b1aeeb81>, line 89)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-0e65b1aeeb81>\"\u001b[0;36m, line \u001b[0;32m89\u001b[0m\n\u001b[0;31m    info[]\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Let's assume that you combined the code from the previous 2 exercises with code\n",
    "from the lesson on how to build requests, and downloaded all the data locally.\n",
    "The files are in a directory \"data\", named after the carrier and airport:\n",
    "\"{}-{}.html\".format(carrier, airport), for example \"FL-ATL.html\".\n",
    "\n",
    "The table with flight info has a table class=\"dataTDRight\". Your task is to\n",
    "use 'process_file()' to extract the flight data from that table as a list of\n",
    "dictionaries, each dictionary containing relevant data from the file and table\n",
    "row. This is an example of the data structure you should return:\n",
    "\n",
    "data = [{\"courier\": \"FL\",\n",
    "         \"airport\": \"ATL\",\n",
    "         \"year\": 2012,\n",
    "         \"month\": 12,\n",
    "         \"flights\": {\"domestic\": 100,\n",
    "                     \"international\": 100}\n",
    "        },\n",
    "         {\"courier\": \"...\"}\n",
    "]\n",
    "\n",
    "Note - year, month, and the flight data should be integers.\n",
    "You should skip the rows that contain the TOTAL data for a year.\n",
    "\n",
    "There are couple of helper functions to deal with the data files.\n",
    "Please do not change them for grading purposes.\n",
    "All your changes should be in the 'process_file()' function.\n",
    "\n",
    "The 'data/FL-ATL.html' file in the tab above is only a part of the full data,\n",
    "covering data through 2003. The test() code will be run on the full table, but\n",
    "the given file should provide an example of what you will get.\n",
    "\"\"\"\n",
    "from bs4 import BeautifulSoup\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "\n",
    "datadir = \"data\"\n",
    "\n",
    "\n",
    "def open_zip(datadir):\n",
    "    with ZipFile('{0}.zip'.format(datadir), 'r') as myzip:\n",
    "        myzip.extractall()\n",
    "\n",
    "\n",
    "def process_all(datadir):\n",
    "    files = os.listdir(datadir)\n",
    "    return files\n",
    "\n",
    "\n",
    "def process_file(f):\n",
    "    \"\"\"\n",
    "    This function extracts data from the file given as the function argument in\n",
    "    a list of dictionaries. This is example of the data structure you should\n",
    "    return:\n",
    "\n",
    "    data = [{\"courier\": \"FL\",\n",
    "             \"airport\": \"ATL\",\n",
    "             \"year\": 2012,\n",
    "             \"month\": 12,\n",
    "             \"flights\": {\"domestic\": 100,\n",
    "                         \"international\": 100}\n",
    "            },\n",
    "            {\"courier\": \"...\"}\n",
    "    ]\n",
    "\n",
    "\n",
    "    Note - year, month, and the flight data should be integers.\n",
    "    You should skip the rows that contain the TOTAL data for a year.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    info = {}\n",
    "    info[\"courier\"], info[\"airport\"] = f[:6].split(\"-\")\n",
    "    # Note: create a new dictionary for each entry in the output data list.\n",
    "    # If you use the info dictionary defined here each element in the list \n",
    "    # will be a reference to the same info dictionary.\n",
    "    with open(\"{}/{}\".format(datadir, f), \"r\") as html:\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        relevant_html = soup.find('table', class_='dataTDRight')\n",
    "        all_trs = relevant_html.find_all(\"tr\")\n",
    "        for tr in all_trs:\n",
    "            if 'style' in tr.attrs and tr['style'] == \"color:White;background-color:#5D95C9;\": \n",
    "                pass\n",
    "            elif 'style' in tr.attrs and tr['style'] == \"background-color:LightYellow;\": \n",
    "                pass\n",
    "            else: \n",
    "            \n",
    "                row = []\n",
    "                tr_info = {}\n",
    "                tr_info[\"courier\"] = info[\"courier\"]\n",
    "                tr_info[\"airport\"] = info[\"airport\"]\n",
    "                for cell in tr.find_all(\"td\"):\n",
    "                    row.append(cell.text)\n",
    "                tr_info[\"year\"] = int(float(row[0]))\n",
    "                tr_info[\"month\"] = int(float(row[1]))\n",
    "                tr_info[\"flights\"] = {\n",
    "                                        \"domestic\": int(float(row[2].replace(',',''))),\n",
    "                                        \"international\": int(float(row[3].replace(',','')))}\n",
    "                data.append(tr_info)\n",
    "    return data\n",
    "\n",
    "\n",
    "def test():\n",
    "    print \"Running a simple test...\"\n",
    "    open_zip(datadir)\n",
    "    files = process_all(datadir)\n",
    "    data = []\n",
    "    # Test will loop over three data files.\n",
    "    for f in files:\n",
    "        data += process_file(f)\n",
    "        \n",
    "    assert len(data) == 399  # Total number of rows\n",
    "    for entry in data[:3]:\n",
    "        assert type(entry[\"year\"]) == int\n",
    "        assert type(entry[\"month\"]) == int\n",
    "        assert type(entry[\"flights\"][\"domestic\"]) == int\n",
    "        assert len(entry[\"airport\"]) == 3\n",
    "        assert len(entry[\"courier\"]) == 2\n",
    "    assert data[0][\"courier\"] == 'FL'\n",
    "    assert data[0][\"month\"] == 10\n",
    "    assert data[-1][\"airport\"] == \"ATL\"\n",
    "    assert data[-1][\"flights\"] == {'international': 108289, 'domestic': 701425}\n",
    "    \n",
    "    print \"... success!\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This and the following exercise are using US Patent database. The patent.data\n",
    "file is a small excerpt of much larger datafiles that are available for\n",
    "download from US Patent website. These files are pretty large ( >100 MB each).\n",
    "The original file is ~600MB large, you might not be able to open it in a text\n",
    "editor.\n",
    "\n",
    "The data itself is in XML, however there is a problem with how it's formatted.\n",
    "Please run this script and observe the error. Then find the line that is\n",
    "causing the error. You can do that by just looking at the datafile in the web\n",
    "UI, or programmatically. For quiz purposes it does not matter, but as an\n",
    "exercise we suggest that you try to do it programmatically.\n",
    "\n",
    "NOTE: You do not need to correct the error - for now, just find where the error\n",
    "is occurring.\n",
    "\"\"\"\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "PATENTS = 'patent.data'\n",
    "\n",
    "def get_root(fname):\n",
    "\n",
    "    tree = ET.parse(fname)\n",
    "    return tree.getroot()\n",
    "\n",
    "\n",
    "get_root(PATENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# So, the problem is that the gigantic file is actually not a valid XML, because\n",
    "# it has several root elements, and XML declarations.\n",
    "# It is, a matter of fact, a collection of a lot of concatenated XML documents.\n",
    "# So, one solution would be to split the file into separate documents,\n",
    "# so that you can process the resulting files as valid XML documents.\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "PATENTS = 'patent.data'\n",
    "\n",
    "def get_root(fname):\n",
    "    tree = ET.parse(fname)\n",
    "    return tree.getroot()\n",
    "\n",
    "\n",
    "def split_file(filename):\n",
    "    \"\"\"\n",
    "    Split the input file into separate files, each containing a single patent.\n",
    "    As a hint - each patent declaration starts with the same line that was\n",
    "    causing the error found in the previous exercises.\n",
    "    \n",
    "    The new files should be saved with filename in the following format:\n",
    "    \"{}-{}\".format(filename, n) where n is a counter, starting from 0.\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as f_in:\n",
    "        for line in f_in:\n",
    "            if line.find('xml version=\"1.0\"'):\n",
    "                line.split()\n",
    "    pass\n",
    "\n",
    "\n",
    "def test():\n",
    "    split_file(PATENTS)\n",
    "    for n in range(4):\n",
    "        try:\n",
    "            fname = \"{}-{}\".format(PATENTS, n)\n",
    "            f = open(fname, \"r\")\n",
    "            if not f.readline().startswith(\"<?xml\"):\n",
    "                print \"You have not split the file {} in the correct boundary!\".format(fname)\n",
    "            f.close()\n",
    "        except:\n",
    "            print \"Could not find file {}. Check if the filename is correct!\".format(fname)\n",
    "\n",
    "\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python 3.6]",
   "language": "python",
   "name": "conda-env-Python 3.6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
